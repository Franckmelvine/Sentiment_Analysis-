# ğŸ’¬ Sentiment Analysis Pipeline â€“ MLOps 

[![Tests](https://github.com/Franckmelvine/Sentiment_Analysis-/actions/workflows/test.yml/badge.svg)](https://github.com/Franckmelvine/Sentiment_Analysis-/actions/workflows/test.yml)
[![Evaluate](https://github.com/Franckmelvine/Sentiment_Analysis-/actions/workflows/evaluate.yml/badge.svg)](https://github.com/Franckmelvine/Sentiment_Analysis-/actions/workflows/evaluate.yml)
[![Build](https://github.com/Franckmelvine/Sentiment_Analysis-/actions/workflows/build.yml/badge.svg)](https://github.com/Franckmelvine/Sentiment_Analysis-/actions/workflows/build.yml)
[![Release](https://github.com/Franckmelvine/Sentiment_Analysis-/actions/workflows/release.yml/badge.svg)](https://github.com/Franckmelvine/Sentiment_Analysis-/actions/workflows/release.yml)   

---

## ğŸ¯ Objectif

Ce projet vise Ã  dÃ©ployer un modÃ¨le BERT de dÃ©tection de sentiments dans un pipeline complet de MLOps.  
Il s'agit de la suite directe dâ€™un premier projet oÃ¹ nous avons dÃ©veloppÃ© et testÃ© un modÃ¨le de classification de sentiments.

---

## ğŸ§± Structure du projet

```
sentiment-analysis-pipeline/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ test.yml
â”‚       â”œâ”€â”€ evaluate.yml
â”‚       â”œâ”€â”€ build.yml
â”‚       â””â”€â”€ release.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_extraction.py
â”‚   â”œâ”€â”€ data_processing.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ inference.py
â”‚   â”œâ”€â”€ app.py                   # Interface Streamlit
â”‚   â”œâ”€â”€ api.py                   # API FastAPI
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â””â”€â”€ performance_report.py
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ unit/
â”‚       â”œâ”€â”€ test_model.py
â”‚       â”œâ”€â”€ test_inference.py
â”‚       â”œâ”€â”€ test_data_extraction.py
â”‚       â”œâ”€â”€ test_data_processing.py
â”œâ”€â”€ .flake8
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ Dockerfile.api
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation locale

```bash
git clone https://github.com/Franckmelvine/Sentiment_Analysis-.git
cd Sentiment_Analysis-
python -m venv venv
source venv/bin/activate  # sous Windows : venv\Scripts\activate
pip install -r requirements.txt
```

---

## ğŸ³ ExÃ©cution avec Docker

### ğŸ”§ Lancer Ã  la fois lâ€™API FastAPI et lâ€™interface Streamlit :

```bash
docker compose up --build
```

- ğŸ“ Interface utilisateur streamlit: http://localhost:8000
- ğŸ“ Interface APIFast: http://localhost:8001
  ![image](https://github.com/user-attachments/assets/8b58c12b-e077-4eae-898d-eb7fee87bd17)

---

## ğŸ§ª ExÃ©cution des tests

```bash
pytest
```

> âœ… Tous les tests unitaires sont dans le dossier `tests/unit/`

---

## ğŸ§¬ Ã‰valuation automatique du modÃ¨le

- Script : `src/evaluate.py`
- GÃ©nÃ¨re un fichier `metrics.json` contenant l'accuracy
- Le pipeline GitHub Ã©choue si accuracy < 0.75

---

## ğŸ¨ Interface utilisateur (Streamlit)

```bash
streamlit run src/app.py
```
![image](https://github.com/user-attachments/assets/b348b928-8052-48a0-ae59-cae9e5c37ec7)


Permet de tester manuellement le modÃ¨le avec retour visuel et colorÃ©.

---

## ğŸ› ï¸ GitHub Actions CI/CD

| Workflow        | DÃ©clencheur                  | Description                                      |
|----------------|------------------------------|--------------------------------------------------|
| `test.yml`     | push, pull_request           | ExÃ©cute linting + tests unitaires               |
| `evaluate.yml` | aprÃ¨s succÃ¨s de `test.yml`   | Ã‰value les performances et vÃ©rifie lâ€™accuracy   |
| `build.yml`    | push sur `main`              | Build et push lâ€™image Docker vers GHCR          |
| `release.yml`  | push dâ€™un tag (ex: v1.0.0)   | CrÃ©e automatiquement une release GitHub         |

---

## ğŸ“š Documentation Technique

### ğŸ§  Architecture MLOps

- **ModÃ¨le** : BERT fine-tunÃ© via Hugging Face
- **API** : FastAPI pour servir le modÃ¨le
- **Interface utilisateur** : Streamlit
- **Docker** : pour packager API + interface
- **GitHub Actions** : pour tout automatiser via CI/CD

---

### âš™ï¸ Choix techniques

- ğŸ”¹ **Transformers (HuggingFace)** pour le NLP
- ğŸ”¹ **FastAPI** pour exposer le modÃ¨le en REST
- ğŸ”¹ **Docker** pour une portabilitÃ© maximale
- ğŸ”¹ **GitHub Actions** pour automatiser tests, Ã©valuation, build et release
- ğŸ”¹ **.flake8** pour le style de code

---

### ğŸ”„ Flux de travail automatisÃ©

1. Dev push son code sur GitHub
2. `test.yml` est lancÃ© : lint + tests
3. Si succÃ¨s, `evaluate.yml` est dÃ©clenchÃ© :
   - Ã©value les performances
   - Ã©choue si accuracy < 0.75
   - stocke les rÃ©sultats
4. Si tout est OK, `build.yml` crÃ©e et pousse l'image Docker
5. Une release peut Ãªtre crÃ©Ã©e via un tag Git (ex: `v1.0.0`)

---

## ğŸ“ˆ Monitoring des performances

### Script automatique : `src/performance_report.py`

Permet dâ€™afficher les rÃ©sultats de `metrics.json` :

```bash
python src/performance_report.py
```

Exemple de sortie :
```
ğŸ“Š Rapport de performance :
- Accuracy : 84.20%
```

ğŸ” IntÃ©grÃ© automatiquement dans le workflow `evaluate.yml`.

---

## ğŸ‘¥ Auteurs & RÃ©partition

| Ã‰tudiant      | TÃ¢ches principales                          |
|---------------|---------------------------------------------|
| **Melvine**   | Docker, FastAPI, Streamlit, README, Rapport |
| **Owen**      | GitHub Actions, Tests, Ã‰valuation, CI/CD    |

> ğŸ”„ Collaboration constante via GitHub : pull requests, revue de code, Ã©changes sur les erreurs

---

## ğŸ“„ Rapport de projet

ğŸ“ `Rapport_MLOps_Owen_et_Melvine.pdf` : contient :
- Lâ€™architecture technique complÃ¨te
- Les outils utilisÃ©s
- Les dÃ©fis rencontrÃ©s et solutions
- Les pistes dâ€™amÃ©lioration
- La rÃ©partition des rÃ´les

---

## ğŸ”® AmÃ©liorations futures

- Monitoring Prometheus + Grafana
- Tracking des modÃ¨les avec MLflow
- DÃ©ploiement sur cloud (Railway, Azure, Herokuâ€¦)
- Tests dâ€™intÃ©gration complets sur lâ€™API

---

## ğŸ Merci

Projet rÃ©alisÃ© dans le cadre du module **MLOps**  
ğŸ“† Mars 2025 â€“ *aivancity school for technology business & society*
